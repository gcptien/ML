{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Providing Training Data and Test Data were not matching the questions asking\n",
    "## Code is for general use\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "products = pd.read_csv('amazon_baby.csv',dtype={'name': str, 'review': str, 'rating': float})\n",
    "# products.shape\n",
    "# products.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products.head()\n",
    "products['name'] = products['name'].astype('str')\n",
    "products['review'] = products['review'].astype('str')\n",
    "products = products.fillna({'review':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    translator = str.maketrans(' ',' ', string.punctuation)\n",
    "    return text.translate(translator) \n",
    "\n",
    "\n",
    "# def remove_punctuation2(text):\n",
    "#     import string\n",
    "#     return text.translate(None, string.punctuation) \n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. We will ignore all reviews with rating = 3, since they tend to have a neutral sentiment. \n",
    "\n",
    "products = products[products['rating'] != 3]\n",
    "products.shape\n",
    "products['sentiment'] = products['rating'].apply(lambda x: '+1' if x >3 else '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx = pd.read_json('module-2-assignment-train-idx.json')\n",
    "test_idx = pd.read_json('module-2-assignment-test-idx.json')\n",
    "\n",
    "train_data = products.iloc[train_idx[0],:]\n",
    "test_data = products.iloc[test_idx[0],:]\n",
    "test_d2 = products.iloc[test_idx[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.fillna({'review':''})\n",
    "train_data['review_clean'] = train_data['review'].apply(remove_punctuation)\n",
    "train_data = train_data[train_data['rating'] != 3]\n",
    "train_data.shape\n",
    "train_data['sentiment'] = train_data['rating'].apply(lambda x: '+1' if x >3 else '-1')\n",
    "\n",
    "test_data = test_data.fillna({'review':''})\n",
    "test_data['review_clean'] = test_data['review'].apply(remove_punctuation)\n",
    "test_data = test_data[test_data['rating'] != 3]\n",
    "test_data.shape\n",
    "test_data['sentiment'] = test_data['rating'].apply(lambda x: '+1' if x >3 else '-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the word count vector for each review\n",
    "## 6. We will now compute the word count for each word that appears in the reviews. A vector consisting of word counts is often referred to as bag-of-word features. Since most words occur in only a few reviews, word count vectors are sparse. For this reason, scikit-learn and many other tools use sparse matrices to store a collection of word count vectors. Refer to appropriate manuals to produce sparse word count vectors. General steps for extracting word count vectors are as follows:\n",
    "\n",
    "Learn a vocabulary (set of all words) from the training data. Only the words that show up in the training data will be considered for feature extraction.\n",
    "Compute the occurrences of the words in each review and collect them into a row vector.\n",
    "Build a sparse matrix where each row is the word count vector for the corresponding review. Call this matrix train_matrix.\n",
    "Using the same mapping between words and columns, convert the test data into a sparse matrix test_matrix.\n",
    "The following cell uses CountVectorizer in scikit-learn. Notice the token_pattern argument in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "     # Use this token pattern to keep single-letter words\n",
    "# First, learn vocabulary from the training data and assign columns to words\n",
    "# Then convert the training data into a sparse matrix\n",
    "train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n",
    "# Second, convert the test data into a sparse matrix, using the same word-column mapping\n",
    "test_matrix = vectorizer.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a sentiment classifier with logistic regression\n",
    "### We will now use logistic regression to create a sentiment classifier on the training data.\n",
    "\n",
    "## 7. Learn a logistic regression classifier using the training data. If you are using scikit-learn, you should create an instance of the LogisticRegression class and then call the method fit() to train the classifier. This model should use the sparse word count matrix (train_matrix) as features and the column sentiment of train_data as the target. Use the default values for other parameters. Call this model sentiment_model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sentiment_model = LogisticRegression()\n",
    "sentiment_model.fit(train_matrix, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. There should be over 100,000 coefficients in this sentiment_model. Recall from the lecture that positive weights w_j correspond to weights that cause positive sentiment, while negative weights correspond to negative sentiment. Calculate the number of positive (>= 0, which is actually nonnegative) coefficients.\n",
    "\n",
    "### Quiz question: How many weights are >= 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34518\n"
     ]
    }
   ],
   "source": [
    "positive_sentiment = (sentiment_model.coef_ >= 0).sum()\n",
    "\n",
    "print(positive_sentiment)\n",
    "### A: 68419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions with logistic regression\n",
    "## 9. Now that a model is trained, we can make predictions on the test data. In this section, we will explore this in the context of 3 data points in the test data. Take the 11th, 12th, and 13th data points in the test data and save them to sample_test_data. The following cell extracts the three data points from the SFrame test_data and print their content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_test_data = test_data[10:13]\n",
    "# test_data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_data = test_data[10:13]\n",
    "sample_test_data_2 = sample_test_data.reset_index()\n",
    "sample_test_data_2['review'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_data_2['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sentiment_model.decision_function(test_matrix[8:13])*-1\n",
    "# score = list(score[:,0])\n",
    "# sentiment_model.score\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_prediction(score):\n",
    "    \n",
    "    predictions = []\n",
    "    for i in score:\n",
    "        if i > 0:\n",
    "            prediction =1\n",
    "        elif i <= 0:\n",
    "            prediction =-1\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prediction(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.predict(test_matrix[8:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability Predictions\n",
    "12. Recall from the lectures that we can also calculate the probability predictions from the scores using:\n",
    "\n",
    "$P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w}) = \\dfrac{1}{1+\\exp{(-\\mathbf{w}^\\intercal h(\\mathbf{x}_i))}}$\n",
    "\n",
    "Using the scores calculated previously, write code to calculate the probability that a sentiment is positive using the above formula. For each row, the probabilities should be a number in the range [0, 1].\n",
    "\n",
    "<b>Checkpoint:</b> Make sure your probability predictions match the ones obtained from sentiment_model.\n",
    "\n",
    "<font color =red><b>Quiz question:</b></font> Of the three data points in sample_test_data, which one (first, second, or third) has the lowest probability of being classified as a positive review?\n",
    "\n",
    "<br>\n",
    "Ans: <font color= red>3</font>\n",
    "\n",
    "Quiz: Which of the following products are represented in the 20 most positive reviews?<br>\n",
    "Ans: <font color= red>Britax Decathlon Convertible Car Seat, Tiffany</font>\n",
    "\n",
    "Quiz: Which of the following products are represented in the 20 most negative reviews?\n",
    "\n",
    "Ans: <font color= red>The First Years True Choice P400 Premium Digital Monitor, 2 Parent Unit\n",
    "<br>\n",
    "Peg-Perego Tatamia High Chair, White Latte\n",
    "<br>\n",
    "Safety 1st High-Def Digital Monitor</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_proba(scores):\n",
    "    predictions = []\n",
    "    for score in scores:\n",
    "        prediction = 1/(1+math.exp(-score)) \n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_proba(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['prob'] =sentiment_model.predict_proba(test_matrix)[:,1]\n",
    "test_data.sort_values(by='prob',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  1.00000000e+00,   1.02893199e-19]),\n",
       " array([  1.00000000e+00,   2.48022198e-18]),\n",
       " array([  1.00000000e+00,   3.04595024e-17]),\n",
       " array([  1.00000000e+00,   2.36502267e-18]),\n",
       " array([  1.00000000e+00,   1.74324939e-23]),\n",
       " array([  1.00000000e+00,   1.17021179e-17]),\n",
       " array([  1.00000000e+00,   3.87098039e-24]),\n",
       " array([  1.00000000e+00,   3.37169787e-17]),\n",
       " array([  1.00000000e+00,   5.25578986e-19]),\n",
       " array([  1.00000000e+00,   6.59357792e-22]),\n",
       " array([  1.00000000e+00,   3.56986336e-20]),\n",
       " array([  1.00000000e+00,   2.88833165e-21]),\n",
       " array([  1.00000000e+00,   1.99984805e-17]),\n",
       " array([  1.00000000e+00,   2.47498149e-18]),\n",
       " array([  1.00000000e+00,   3.58030211e-18]),\n",
       " array([  1.00000000e+00,   1.16127055e-18]),\n",
       " array([  1.00000000e+00,   9.93258861e-17]),\n",
       " array([  1.00000000e+00,   1.10778863e-16]),\n",
       " array([  1.00000000e+00,   2.18297239e-16]),\n",
       " array([  1.00000000e+00,   2.78865988e-16])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "predictions = sentiment_model.predict_proba(test_matrix)\n",
    "sorted(predictions,key=itemgetter(0),reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  8.88178420e-16,   1.00000000e+00]),\n",
       " array([  1.99840144e-15,   1.00000000e+00]),\n",
       " array([  8.34887715e-14,   1.00000000e+00]),\n",
       " array([  1.25233157e-13,   1.00000000e+00]),\n",
       " array([  2.52908805e-13,   1.00000000e+00]),\n",
       " array([  3.45057316e-13,   1.00000000e+00]),\n",
       " array([  3.28943539e-11,   1.00000000e+00]),\n",
       " array([  3.29891670e-11,   1.00000000e+00]),\n",
       " array([  9.69757608e-11,   1.00000000e+00]),\n",
       " array([  1.00466302e-10,   1.00000000e+00]),\n",
       " array([  4.38062475e-10,   1.00000000e+00]),\n",
       " array([  4.87991203e-10,   1.00000000e+00]),\n",
       " array([  6.36955377e-10,   9.99999999e-01]),\n",
       " array([  6.48503251e-10,   9.99999999e-01]),\n",
       " array([  6.57814025e-10,   9.99999999e-01]),\n",
       " array([  6.59188926e-10,   9.99999999e-01]),\n",
       " array([  8.18745516e-10,   9.99999999e-01]),\n",
       " array([  1.09411880e-09,   9.99999999e-01]),\n",
       " array([  1.52924851e-09,   9.99999998e-01]),\n",
       " array([  1.74532766e-09,   9.99999998e-01])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(predictions,key=itemgetter(0),reverse=False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by\n",
    "\n",
    "$accuracy=\\frac{\\textrm{# correctly classified examples}}{# total examples}$\n",
    "\n",
    "This can be computed as follows:\n",
    "\n",
    "- Step 1: Use the sentiment_model to compute class predictions.\n",
    "- Step 2: Count the number of data points when the predicted class labels match the ground truth labels.\n",
    "- Step 3: Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "### Quiz Question: What is the accuracy of the sentiment_model on the test_data? Round your answer to 2 decimal places (e.g. 0.76).\n",
    "0.91\n",
    "### Quiz Question: Does a higher accuracy value on the training_data always imply that the classifier is better?\n",
    "NO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sentiment_model.predict(test_matrix)\n",
    "test_data['prediction'] = prediction\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quiz Question: What is the accuracy of the sentiment_model on the test_data? Round your answer to 2 decimal places \n",
    "## (e.g. 0.76).\n",
    "\n",
    "accuracy = sentiment_model.score(test_matrix, test_data['sentiment'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 16. There were a lot of words in the model we trained above. We will now train a simpler logistic regression model using only a subet of words that occur in the reviews. For this assignment, we selected 20 words to work with. These are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']\n",
    "\n",
    "vectorizer_word_subset = CountVectorizer(vocabulary=significant_words) # limit to 20 words\n",
    "train_matrix_word_subset = vectorizer_word_subset.fit_transform(train_data['review_clean'])\n",
    "test_matrix_word_subset = vectorizer_word_subset.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = LogisticRegression()\n",
    "simple_model.fit(train_matrix_word_subset,train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model_coef_table = pd.DataFrame({'word':significant_words,\n",
    "                                         'coefficient':simple_model.coef_[0,:]*-1})\n",
    "# simple_model.coef_\n",
    "# len(significant_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.673074</td>\n",
       "      <td>loves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.509812</td>\n",
       "      <td>perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.363690</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.192538</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.944000</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.520186</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.503760</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.190909</td>\n",
       "      <td>able</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085513</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.058855</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.209563</td>\n",
       "      <td>less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.320556</td>\n",
       "      <td>product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.362167</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.511380</td>\n",
       "      <td>even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.621169</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.898031</td>\n",
       "      <td>money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.651576</td>\n",
       "      <td>broke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-2.033699</td>\n",
       "      <td>waste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-2.109331</td>\n",
       "      <td>return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-2.348298</td>\n",
       "      <td>disappointed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    coefficient          word\n",
       "6      1.673074         loves\n",
       "5      1.509812       perfect\n",
       "0      1.363690          love\n",
       "2      1.192538          easy\n",
       "1      0.944000         great\n",
       "4      0.520186        little\n",
       "7      0.503760          well\n",
       "8      0.190909          able\n",
       "3      0.085513           old\n",
       "9      0.058855           car\n",
       "11    -0.209563          less\n",
       "16    -0.320556       product\n",
       "18    -0.362167         would\n",
       "12    -0.511380          even\n",
       "15    -0.621169          work\n",
       "17    -0.898031         money\n",
       "10    -1.651576         broke\n",
       "13    -2.033699         waste\n",
       "19    -2.109331        return\n",
       "14    -2.348298  disappointed"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model_coef_table.sort_values(by='coefficient' ,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz Question: Consider the coefficients of simple_model. How many of the 20 coefficients (corresponding to the 20 significant_words) are positive for the simple_model?\n",
    "\n",
    "### Quiz Question: Are the positive words in the simple_model also positive words in the sentiment_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coefficient    10\n",
       "word           10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model_coef_table[simple_model_coef_table['coefficient']>0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(simple_model_coef_table['coefficient']>0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. We will now compare the accuracy of the sentiment_model and the simple_model.\n",
    "\n",
    "First, compute the classification accuracy of the sentiment_model on the train_data.\n",
    "\n",
    "Now, compute the classification accuracy of the simple_model on the train_data.\n",
    "\n",
    "### Quiz Question: Which model (sentiment_model or simple_model) has higher accuracy on the TRAINING set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968624452839\n",
      "0.866822570007\n"
     ]
    }
   ],
   "source": [
    "m_train_score = sentiment_model.score(train_matrix,train_data['sentiment'])\n",
    "s_train_score = simple_model.score(train_matrix_word_subset,train_data['sentiment'])\n",
    "print(m_train_score)\n",
    "print(s_train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Now, we will repeat this exercise on the test_data. Start by computing the classification accuracy of the sentiment_model on the test_data.\n",
    "\n",
    "Next, compute the classification accuracy of the simple_model on the test_data.\n",
    "\n",
    "### Quiz Question: Which model (sentiment_model or simple_model) has higher accuracy on the TEST set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932415406767\n",
      "0.869360451164\n"
     ]
    }
   ],
   "source": [
    "m_test_score = sentiment_model.score(test_matrix,test_data['sentiment'])\n",
    "s_test_score = simple_model.score(test_matrix_word_subset,test_data['sentiment'])\n",
    "print(m_test_score)\n",
    "print(s_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. It is quite common to use the majority class classifier as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "### Quiz Question: Enter the accuracy of the majority class classifier model on the test_data. Round your answer to two decimal places (e.g. 0.76).\n",
    "0.84\n",
    "\n",
    "### Quiz Question: Is the sentiment_model definitely better than the majority class classifier (the baseline)?\n",
    "Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28095"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sent = (test_data['sentiment']=='+1').sum()\n",
    "\n",
    "positive_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84278257739380846"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sent/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
